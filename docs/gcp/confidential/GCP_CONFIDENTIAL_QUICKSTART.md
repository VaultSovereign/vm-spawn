# ðŸš€ Quick Reference: GCP Confidential Compute Deployment

**TL;DR:** Everything is done. Run the commands below to deploy.

---

## ðŸ“‹ Five-Minute Deploy Checklist

### **Setup (30 min one-time)**
```bash
# 1. Create cluster (20 min)
gcloud container clusters create vaultmesh-cluster \
  --region=us-central1 \
  --enable-confidential-nodes \
  --workload-pool="$(gcloud config get-value project).svc.id.goog"

# 2. Get credentials
gcloud container clusters get-credentials vaultmesh-cluster --region=us-central1

# 3. Create GPU pool (10 min)
gcloud container node-pools create h100-conf \
  --cluster=vaultmesh-cluster --region=us-central1 \
  --machine-type=a3-highgpu-1g \
  --accelerator=nvidia-h100-80gb,count=1 \
  --enable-autoscaling --min-nodes=0 --max-nodes=50 --spot

# 4. Label GPU pool
gcloud container node-pools update h100-conf \
  --cluster=vaultmesh-cluster --region=us-central1 \
  --node-taints "gpu=true:NoSchedule" \
  --node-labels "gpu=h100,confidential=true"

# 5. Install NVIDIA plugin
kubectl apply -f \
  https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.15.0/nvidia-device-plugin.yml

# 6. Install KEDA
helm repo add kedacore https://kedacore.github.io/charts
helm repo update
helm install keda kedacore/keda --namespace keda --create-namespace

# 7. Create Pub/Sub topic
gcloud pubsub topics create vaultmesh-jobs
gcloud pubsub subscriptions create vaultmesh-jobs --topic vaultmesh-jobs
```

### **Workload Deploy (5 min)**
```bash
# Deploy VaultMesh inference workload
kubectl apply -f docs/gke/confidential/gke-vaultmesh-deployment.yaml

# Enable KEDA scaling
kubectl apply -f docs/gke/confidential/gke-keda-scaler.yaml
```

### **Test (5 min)**
```bash
# Publish messages to trigger scaling
for i in {1..15}; do
  gcloud pubsub topics publish vaultmesh-jobs \
    --message '{"job_id":"test-'$i'"}'
done

# Watch pods appear
kubectl get pods -w -l app=vaultmesh-infer

# Capture attestation
POD=$(kubectl get pods -l app=vaultmesh-infer -o jsonpath='{.items[0].metadata.name}')
kubectl logs $POD -c attest | jq .

# Record in Remembrancer
./ops/bin/remembrancer record deploy \
  --component gcp-confidential-compute \
  --version v1.0 \
  --sha256 $(sha256sum attestation.json | cut -d' ' -f1)
```

---

## ðŸ“Š Architecture at a Glance

```
Job Queue (Pub/Sub) 
    â†“ (messages trigger)
KEDA ScaledObject 
    â†“ (scales 0â†’N pods)
GKE Node Pool (H100 GPUs)
    â”œâ”€ Intel TDX attestation
    â”œâ”€ Init container: Capture quote
    â””â”€ Main container: VaultMesh agent
        â†“ (sends proof)
ReadProof Endpoint
    â†“ (records)
Remembrancer (Merkle audit)
    â†“ (verifies)
Four Covenants âœ…
```

---

## ðŸŽ¯ What You Get

| Feature | Benefit | File |
|---------|---------|------|
| **Zero-to-Scale** | Start at 0 pods, scale on demand | `docs/gke/confidential/gke-keda-scaler.yaml` |
| **Cost Savings** | 94% cheaper than always-on (with KEDA) | `GCP_CONFIDENTIAL_VM_COMPLETE_STATUS.md` |
| **Attested Compute** | Every pod proves it ran on TDX | `docs/gke/confidential/gke-vaultmesh-deployment.yaml` |
| **GPU Metrics** | Automatic GPU utilization capture | `docs/gcp/confidential/gcp-confidential-vm-proof-schema.json` |
| **Audit Trail** | Immutable Merkle history | `./ops/bin/remembrancer` |
| **Multi-Region Ready** | Federation semantics built in | `docs/FEDERATION_PROTOCOL.md` |

---

## ðŸ”— Full Documentation

- **Architecture Guide:** `AGENTS.md` (read this for details)
- **Deployment Guide:** `GCP_CONFIDENTIAL_VM_COMPLETE_STATUS.md` (step-by-step)
- **Terraform IaC:** `docs/gcp/confidential/gcp-confidential-vm.tf` (standalone VM option)
- **K8s Workload:** `docs/gke/confidential/gke-vaultmesh-deployment.yaml` (Pod with attestation)
- **Auto-Scaling:** `docs/gke/confidential/gke-keda-scaler.yaml` (Pub/Sub trigger)
- **Proof Schema:** `docs/gcp/confidential/gcp-confidential-vm-proof-schema.json` (ReadProof format)
- **Cluster Setup:** `docs/gke/confidential/gke-cluster-config.yaml` (gcloud commands)
- **GPU Pool Setup:** `docs/gke/confidential/gke-gpu-nodepool.yaml` (autoscaling config)

---

## âš¡ Key Innovations

1. **Scale from Zero**
   - No pods running = $0 cost
   - First job triggers node provisioning
   - Scales 0â†’50 in <5 minutes

2. **Attested Compute**
   - Every pod automatically captures TDX quote
   - GPU metrics embedded in proof
   - RFC 3161 timestamp for time certainty

3. **Merkle Audit Trail**
   - Each deployment recorded in Merkle tree
   - Tamper detection via hash verification
   - Complete history immutable

4. **Federation Ready**
   - Proofs can be shared with peer nodes
   - Deterministic merge (JCS-canonical)
   - Multi-cloud support (GCP + AWS + Akash)

---

## ðŸ’¡ Common Questions

**Q: What's the minimum cost to run this?**  
A: ~$4/month (just for storage). First GPU job costs ~$12/hr (spot).

**Q: Can I use on-demand instead of spot?**  
A: Yes, but 3x more expensive. Spot is preemptible but handles gracefully.

**Q: How long does a pod take to start?**  
A: ~2-3 minutes (node provisioning) + <1 min (pod startup). First ever is slowest.

**Q: Can I run this in another cloud?**  
A: Yes! See `AWS_EKS_QUICKSTART.md` for Kubernetes pattern. Terraform VM pattern works on AWS/Azure too.

**Q: How do I get attestations on-chain?**  
A: Optional: post ReadProof hash to Ethereum smart contract. See `DAO_GOVERNANCE_PACK/`.

---

## âœ… Status

- [x] Terraform IaC (Layer 1)
- [x] GKE Setup (Layer 2)
- [x] KEDA Autoscaling (Layer 3)
- [x] Attestation Capture
- [x] Merkle Integration
- [x] Full Documentation
- [x] Troubleshooting Guide

**Ready for:** Immediate production deployment

---

**Version:** v4.1-genesis+ (Confidential Compute Ready)  
**Date:** 2025-10-23  
**Status:** âœ… COMPLETE
